{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-moore",
   "metadata": {},
   "source": [
    "# The QR Decomposition for radial neural networks\n",
    "\n",
    "This repository accompanies the paper \"The QR decomposition for radial neural networks\".\n",
    "\n",
    "## The file $\\texttt{source.py}$\n",
    "\n",
    "### The representation class\n",
    "\n",
    "In the file $\\texttt{source.py}$, we create a class $\\texttt{representation}$ for representations of the neural quiver $\\mathscr{Q}_L$, which is the following quiver:\n",
    "<img src=\"neural-quiver.png\" alt=\"drawing\" width=\"500\"/>\n",
    "with $L + 1$ vertices in the top row and a 'bias vertex' at the bottom. We only consider dimension vectors whose value at the bias vertex is equal to $1$, so a dimension vector for $\\mathscr{Q}_L$ refers to a tuple $\\mathbf{n} = (n_0, n_1, \\dots, n_L)$. For any such dimension vector, the vector space of representations of the neural quiver $\\mathscr{Q}_L$ can be identified with a direct sum of matrix spaces:\n",
    "$$\\mathsf{Rep}(\\mathscr{Q}_L, \\mathbf{n})  \\simeq  \\bigoplus_{i=1}^L \\mathrm{Hom}(\\mathbb{R}^{1 +n_{i-1}}, \\mathbb{R}^{n_i})$$\n",
    "where, $\\mathrm{Hom}(\\mathbb{R}^{1 +n_{i-1}}, \\mathbb{R}^{n_i})$ denotes the space of $n_i \\times (1+n_{i-1}$ matrices, i.e. the space of linear maps from $\\mathbb{R}^{n_i}$ to $\\mathbb{R}^{1 + n_{i-1}}$. \n",
    "\n",
    "Let  $\\mathbf{W} = (W_i)_{i=1}^L$ be a representation of the neural quiver with dimension vector $\\mathbf{n}$, so each $W_i$ is an $n_i \\times (1 + n_{i-1})$ matrix. The class $\\texttt{representation}$ includes methods to compute:\n",
    "\n",
    "-- The QR decomposition of the representation $\\mathbf{W}$, so $$\\mathbf{W} = \\mathbf{Q} \\cdot \\mathbf{R} + \\mathbf{U}.$$\n",
    "\n",
    "-- The reduced representation $\\mathbf{R}$.\n",
    "\n",
    "-- The transformed representation $\\mathbf{Q}^{-1} \\cdot \\mathbf{W}$.\n",
    "\n",
    "### Radial neural networks\n",
    "\n",
    "Continuing in the $\\texttt{source.py}$, we use PyTorch modules to implement radial activation functions in the class $\\texttt{RadAct(nn.Module)}$, where there is an option to add a shift. We then implement radial neural networks in the class $\\texttt{RadNet(nn.Module)}$, which has methods to:\n",
    "\n",
    "-- Set the weights.\n",
    "\n",
    "-- Export the weights $\\mathbf{W}$.\n",
    "\n",
    "-- Export the reduced network (with weights $\\mathbf{R}$ from the QR decomposition for $\\mathbf{W}$).\n",
    "\n",
    "-- Export the transformed network (with weights $\\mathbf{Q}^{-1} \\cdot \\mathbf{W}$ from the QR decomposition for $\\mathbf{W}$).\n",
    "\n",
    "### Training\n",
    "\n",
    "For training models, we have three different types of training loops:\n",
    "\n",
    "-- $\\texttt{training_loop}$, which is the most basic training loop with usual gradient descent. There is no optimizer in order to remove randomness. \n",
    "\n",
    "-- $\\texttt{training_loop_proj_GD}$, which uses projected gradient descent. We define the appropriate masks in order to implement this properly. \n",
    "\n",
    "-- $\\texttt{training_loop_with_stop}$, which impelments usual gradient descent, but with a stopping value for the loss function\n",
    "\n",
    "## The file $\\texttt{script-experiment-1.py}$\n",
    "\n",
    "In this experiment, we instantiate a radial neural network with weights $\\mathbf{W}$ and show that projected gradient descent on the transformed network (with weights $\\mathbf{Q}^{-1} \\cdot \\mathbf{W}$) matches usual gradient descent on the reduced network (with weights $\\mathbf{R}$). Specifically, the values of the loss function are the same in both training regimes, epoch by epoch. \n",
    "\n",
    "\n",
    "## The file $\\texttt{script-experiment-2.py}$\n",
    "\n",
    "In this experiment, we instantiate a radial neural network with weights $\\mathbf{W}$ and a somewhat large dimension vector. We train both the original model and the reduced model (with weights $\\mathbf{R}$ coming from the QR decomposition of $\\mathbf{W}$) with usual gradient descent using a stopping value for the loss function. We show that the reduced model achieves this low value for the loss function after less time (albeit after more epochs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-simon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blind-horizon",
   "metadata": {},
   "source": [
    "# The QR Decomposition for radial neural networks\n",
    "\n",
    "This repository accompanies the paper \"The QR decomposition for radial neural networks\".\n",
    "\n",
    "## The file source.py\n",
    "\n",
    "### The representation class\n",
    "\n",
    "We create a class for representations of the neural quiver:\n",
    "<img src=\"neural-quiver.png\" alt=\"drawing\" width=\"500\"/>\n",
    "with L + 1 vertices in the top row and a 'bias vertex' at the bottom. We only consider dimension vectors whose value at the bias vertex is equal to 1.  \n",
    "\n",
    "-- The QR decomposition of the representation W, that is, W = QR + U\n",
    "\n",
    "-- The reduced representation (R from the QR decomposition)\n",
    "\n",
    "-- The transformed representation (Q^{-1} W where Q is from the QR decomposition)\n",
    "\n",
    "\n",
    "### Radial neural networks\n",
    "\n",
    "We use PyTorch modules to implement radial activation functions in the class RadAct where there is an option to add a shift. We then implement radial neural networks in the class RadNet which has methods to:\n",
    "\n",
    "-- Set the weights.\n",
    "\n",
    "-- Export the weights 'W'.\n",
    "\n",
    "-- Export the reduced network (with weights R from the QR decomposition for the weights W).\n",
    "\n",
    "-- Export the transformed network (with weights Q^{-1} W where Q is from the QR decomposition for W).\n",
    "\n",
    "### Training\n",
    "\n",
    "For training models, we have three different types of training loops:\n",
    "\n",
    "-- A basic training loop with usual gradient descent. There is no optimizer in order to remove randomness. \n",
    "\n",
    "-- A training loop with projected gradient descent. We define the appropriate masks in order to implement this properly. \n",
    "\n",
    "-- A training loop with usual gradient descent and a stopping value for the loss function\n",
    "\n",
    "## The file script-experiment-1.py\n",
    "\n",
    "In this experiment, we instantiate a radial neural network with weights W and show that projected gradient descent on the transformed network (with weights Q^{-1} W) matches usual gradient descent on the reduced network (with weights R). Specifically, the values of the loss function are the same in both training regimes, epoch by epoch. \n",
    "\n",
    "\n",
    "## The file script-experiment-2.py\n",
    "\n",
    "In this experiment, we instantiate a radial neural network with weights W and a somewhat large dimension vector. We train both the original model and the reduced model (with weights R coming from the QR decomposition of W) with usual gradient descent using a stopping value for the loss function. We show that the reduced model achieves this low value for the loss function after less time (albeit after more epochs).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let  $\\mathbf{W} = (W_i)_{i=1}^L$ be a representation of the neural quiver with dimension vector $\\mathbf{n}$, so each $W_i$ is an $n_i \\times (1 + n_{i-1})$ matrix. The class $\\texttt{representation}$ includes methods to compute:\n",
    "\n",
    "\n",
    "\n",
    "so a dimension vector for $\\mathscr{Q}_L$ refers to a tuple $\\mathbf{n} = (n_0, n_1, \\dots, n_L)$. For any such dimension vector, the vector space of representations of the neural quiver $\\mathscr{Q}_L$ can be identified with a direct sum of matrix spaces:\n",
    "$$\\mathsf{Rep}(\\mathscr{Q}_L, \\mathbf{n})  \\simeq  \\bigoplus_{i=1}^L \\mathrm{Hom}(\\mathbb{R}^{1 +n_{i-1}}, \\mathbb{R}^{n_i})$$\n",
    "where, $\\mathrm{Hom}(\\mathbb{R}^{1 +n_{i-1}}, \\mathbb{R}^{n_i})$ denotes the space of $n_i \\times (1+n_{i-1}$ matrices, i.e. the space of linear maps from $\\mathbb{R}^{n_i}$ to $\\mathbb{R}^{1 + n_{i-1}}$. \n",
    "\n",
    "Let  $\\mathbf{W} = (W_i)_{i=1}^L$ be a representation of the neural quiver with dimension vector $\\mathbf{n}$, so each $W_i$ is an $n_i \\times (1 + n_{i-1})$ matrix. The class $\\texttt{representation}$ includes methods to compute:\n",
    "\n",
    "-- The QR decomposition of the representation $\\mathbf{W}$, so $$\\mathbf{W} = \\mathbf{Q} \\cdot \\mathbf{R} + \\mathbf{U}.$$\n",
    "\n",
    "-- The reduced representation $\\mathbf{R}$.\n",
    "\n",
    "-- The transformed representation $\\mathbf{Q}^{-1} \\cdot \\mathbf{W}$.\n",
    "\n",
    "### Radial neural networks\n",
    "\n",
    "Continuing in the $\\texttt{source.py}$, we use PyTorch modules to implement radial activation functions in the class $\\texttt{RadAct(nn.Module)}$, where there is an option to add a shift. We then implement radial neural networks in the class $\\texttt{RadNet(nn.Module)}$, which has methods to:\n",
    "\n",
    "-- Set the weights.\n",
    "\n",
    "-- Export the weights $\\mathbf{W}$.\n",
    "\n",
    "-- Export the reduced network (with weights $\\mathbf{R}$ from the QR decomposition for $\\mathbf{W}$).\n",
    "\n",
    "-- Export the transformed network (with weights $\\mathbf{Q}^{-1} \\cdot \\mathbf{W}$ from the QR decomposition for $\\mathbf{W}$).\n",
    "\n",
    "### Training\n",
    "\n",
    "For training models, we have three different types of training loops:\n",
    "\n",
    "-- $\\texttt{training_loop}$, which is the most basic training loop with usual gradient descent. There is no optimizer in order to remove randomness. \n",
    "\n",
    "-- $\\texttt{training_loop_proj_GD}$, which uses projected gradient descent. We define the appropriate masks in order to implement this properly. \n",
    "\n",
    "-- $\\texttt{training_loop_with_stop}$, which impelments usual gradient descent, but with a stopping value for the loss function\n",
    "\n",
    "## The file $\\texttt{script-experiment-1.py}$\n",
    "\n",
    "In this experiment, we instantiate a radial neural network with weights $\\mathbf{W}$ and show that projected gradient descent on the transformed network (with weights $\\mathbf{Q}^{-1} \\cdot \\mathbf{W}$) matches usual gradient descent on the reduced network (with weights $\\mathbf{R}$). Specifically, the values of the loss function are the same in both training regimes, epoch by epoch. \n",
    "\n",
    "\n",
    "## The file $\\texttt{script-experiment-2.py}$\n",
    "\n",
    "In this experiment, we instantiate a radial neural network with weights $\\mathbf{W}$ and a somewhat large dimension vector. We train both the original model and the reduced model (with weights $\\mathbf{R}$ coming from the QR decomposition of $\\mathbf{W}$) with usual gradient descent using a stopping value for the loss function. We show that the reduced model achieves this low value for the loss function after less time (albeit after more epochs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-bosnia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-convention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
